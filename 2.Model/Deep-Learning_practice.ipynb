{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 과제 (유방암 사례)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_data = bc.data\n",
    "bc_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, 1.001e+03, 1.184e-01, 2.776e-01,\n",
       "        3.001e-01, 1.471e-01, 2.419e-01, 7.871e-02, 1.095e+00, 9.053e-01,\n",
       "        8.589e+00, 1.534e+02, 6.399e-03, 4.904e-02, 5.373e-02, 1.587e-02,\n",
       "        3.003e-02, 6.193e-03, 2.538e+01, 1.733e+01, 1.846e+02, 2.019e+03,\n",
       "        1.622e-01, 6.656e-01, 7.119e-01, 2.654e-01, 4.601e-01, 1.189e-01],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, 1.326e+03, 8.474e-02, 7.864e-02,\n",
       "        8.690e-02, 7.017e-02, 1.812e-01, 5.667e-02, 5.435e-01, 7.339e-01,\n",
       "        3.398e+00, 7.408e+01, 5.225e-03, 1.308e-02, 1.860e-02, 1.340e-02,\n",
       "        1.389e-02, 3.532e-03, 2.499e+01, 2.341e+01, 1.588e+02, 1.956e+03,\n",
       "        1.238e-01, 1.866e-01, 2.416e-01, 1.860e-01, 2.750e-01, 8.902e-02],\n",
       "       [1.969e+01, 2.125e+01, 1.300e+02, 1.203e+03, 1.096e-01, 1.599e-01,\n",
       "        1.974e-01, 1.279e-01, 2.069e-01, 5.999e-02, 7.456e-01, 7.869e-01,\n",
       "        4.585e+00, 9.403e+01, 6.150e-03, 4.006e-02, 3.832e-02, 2.058e-02,\n",
       "        2.250e-02, 4.571e-03, 2.357e+01, 2.553e+01, 1.525e+02, 1.709e+03,\n",
       "        1.444e-01, 4.245e-01, 4.504e-01, 2.430e-01, 3.613e-01, 8.758e-02],\n",
       "       [1.142e+01, 2.038e+01, 7.758e+01, 3.861e+02, 1.425e-01, 2.839e-01,\n",
       "        2.414e-01, 1.052e-01, 2.597e-01, 9.744e-02, 4.956e-01, 1.156e+00,\n",
       "        3.445e+00, 2.723e+01, 9.110e-03, 7.458e-02, 5.661e-02, 1.867e-02,\n",
       "        5.963e-02, 9.208e-03, 1.491e+01, 2.650e+01, 9.887e+01, 5.677e+02,\n",
       "        2.098e-01, 8.663e-01, 6.869e-01, 2.575e-01, 6.638e-01, 1.730e-01],\n",
       "       [2.029e+01, 1.434e+01, 1.351e+02, 1.297e+03, 1.003e-01, 1.328e-01,\n",
       "        1.980e-01, 1.043e-01, 1.809e-01, 5.883e-02, 7.572e-01, 7.813e-01,\n",
       "        5.438e+00, 9.444e+01, 1.149e-02, 2.461e-02, 5.688e-02, 1.885e-02,\n",
       "        1.756e-02, 5.115e-03, 2.254e+01, 1.667e+01, 1.522e+02, 1.575e+03,\n",
       "        1.374e-01, 2.050e-01, 4.000e-01, 1.625e-01, 2.364e-01, 7.678e-02],\n",
       "       [1.245e+01, 1.570e+01, 8.257e+01, 4.771e+02, 1.278e-01, 1.700e-01,\n",
       "        1.578e-01, 8.089e-02, 2.087e-01, 7.613e-02, 3.345e-01, 8.902e-01,\n",
       "        2.217e+00, 2.719e+01, 7.510e-03, 3.345e-02, 3.672e-02, 1.137e-02,\n",
       "        2.165e-02, 5.082e-03, 1.547e+01, 2.375e+01, 1.034e+02, 7.416e+02,\n",
       "        1.791e-01, 5.249e-01, 5.355e-01, 1.741e-01, 3.985e-01, 1.244e-01],\n",
       "       [1.825e+01, 1.998e+01, 1.196e+02, 1.040e+03, 9.463e-02, 1.090e-01,\n",
       "        1.127e-01, 7.400e-02, 1.794e-01, 5.742e-02, 4.467e-01, 7.732e-01,\n",
       "        3.180e+00, 5.391e+01, 4.314e-03, 1.382e-02, 2.254e-02, 1.039e-02,\n",
       "        1.369e-02, 2.179e-03, 2.288e+01, 2.766e+01, 1.532e+02, 1.606e+03,\n",
       "        1.442e-01, 2.576e-01, 3.784e-01, 1.932e-01, 3.063e-01, 8.368e-02],\n",
       "       [1.371e+01, 2.083e+01, 9.020e+01, 5.779e+02, 1.189e-01, 1.645e-01,\n",
       "        9.366e-02, 5.985e-02, 2.196e-01, 7.451e-02, 5.835e-01, 1.377e+00,\n",
       "        3.856e+00, 5.096e+01, 8.805e-03, 3.029e-02, 2.488e-02, 1.448e-02,\n",
       "        1.486e-02, 5.412e-03, 1.706e+01, 2.814e+01, 1.106e+02, 8.970e+02,\n",
       "        1.654e-01, 3.682e-01, 2.678e-01, 1.556e-01, 3.196e-01, 1.151e-01],\n",
       "       [1.300e+01, 2.182e+01, 8.750e+01, 5.198e+02, 1.273e-01, 1.932e-01,\n",
       "        1.859e-01, 9.353e-02, 2.350e-01, 7.389e-02, 3.063e-01, 1.002e+00,\n",
       "        2.406e+00, 2.432e+01, 5.731e-03, 3.502e-02, 3.553e-02, 1.226e-02,\n",
       "        2.143e-02, 3.749e-03, 1.549e+01, 3.073e+01, 1.062e+02, 7.393e+02,\n",
       "        1.703e-01, 5.401e-01, 5.390e-01, 2.060e-01, 4.378e-01, 1.072e-01],\n",
       "       [1.246e+01, 2.404e+01, 8.397e+01, 4.759e+02, 1.186e-01, 2.396e-01,\n",
       "        2.273e-01, 8.543e-02, 2.030e-01, 8.243e-02, 2.976e-01, 1.599e+00,\n",
       "        2.039e+00, 2.394e+01, 7.149e-03, 7.217e-02, 7.743e-02, 1.432e-02,\n",
       "        1.789e-02, 1.008e-02, 1.509e+01, 4.068e+01, 9.765e+01, 7.114e+02,\n",
       "        1.853e-01, 1.058e+00, 1.105e+00, 2.210e-01, 4.366e-01, 2.075e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_data[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_label = bc.target\n",
    "bc_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "print(bc.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['malignant', 'benign'], dtype='<U9')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  label  \n",
       "0          0.4601                  0.11890      0  \n",
       "1          0.2750                  0.08902      0  \n",
       "2          0.3613                  0.08758      0  \n",
       "3          0.6638                  0.17300      0  \n",
       "4          0.2364                  0.07678      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_df = pd.DataFrame(data=bc_data, columns=bc.feature_names)\n",
    "bc_df['label'] = bc.target\n",
    "dataset = bc_df.values\n",
    "X = dataset[:,0:30]\n",
    "Y = dataset[:,30]\n",
    "bc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>...</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>0.627417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>...</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.483918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>...</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>...</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>...</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>...</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter    mean area  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       mean symmetry  mean fractal dimension  ...  worst texture  \\\n",
       "count     569.000000              569.000000  ...     569.000000   \n",
       "mean        0.181162                0.062798  ...      25.677223   \n",
       "std         0.027414                0.007060  ...       6.146258   \n",
       "min         0.106000                0.049960  ...      12.020000   \n",
       "25%         0.161900                0.057700  ...      21.080000   \n",
       "50%         0.179200                0.061540  ...      25.410000   \n",
       "75%         0.195700                0.066120  ...      29.720000   \n",
       "max         0.304000                0.097440  ...      49.540000   \n",
       "\n",
       "       worst perimeter   worst area  worst smoothness  worst compactness  \\\n",
       "count       569.000000   569.000000        569.000000         569.000000   \n",
       "mean        107.261213   880.583128          0.132369           0.254265   \n",
       "std          33.602542   569.356993          0.022832           0.157336   \n",
       "min          50.410000   185.200000          0.071170           0.027290   \n",
       "25%          84.110000   515.300000          0.116600           0.147200   \n",
       "50%          97.660000   686.500000          0.131300           0.211900   \n",
       "75%         125.400000  1084.000000          0.146000           0.339100   \n",
       "max         251.200000  4254.000000          0.222600           1.058000   \n",
       "\n",
       "       worst concavity  worst concave points  worst symmetry  \\\n",
       "count       569.000000            569.000000      569.000000   \n",
       "mean          0.272188              0.114606        0.290076   \n",
       "std           0.208624              0.065732        0.061867   \n",
       "min           0.000000              0.000000        0.156500   \n",
       "25%           0.114500              0.064930        0.250400   \n",
       "50%           0.226700              0.099930        0.282200   \n",
       "75%           0.382900              0.161400        0.317900   \n",
       "max           1.252000              0.291000        0.663800   \n",
       "\n",
       "       worst fractal dimension       label  \n",
       "count               569.000000  569.000000  \n",
       "mean                  0.083946    0.627417  \n",
       "std                   0.018061    0.483918  \n",
       "min                   0.055040    0.000000  \n",
       "25%                   0.071460    0.000000  \n",
       "50%                   0.080040    1.000000  \n",
       "75%                   0.092080    1.000000  \n",
       "max                   0.207500    1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = \\\n",
    "    train_test_split(X, Y, test_size=0.2, shuffle=True, stratify=Y, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                372       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 1,415\n",
      "Trainable params: 1,415\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(30, input_dim=30, activation='relu'),\n",
    "    Dense(12, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "]) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = MODEL_DIR + \"best{epoch:03d}-{val_loss:.4f}.hdf5\"\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', \n",
    "                               verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 18.80775, saving model to ./model/best001-18.8078.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 18.80775 to 5.84698, saving model to ./model/best002-5.8470.hdf5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 5.84698\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 5.84698\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 5.84698\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 5.84698\n",
      "\n",
      "Epoch 00007: val_loss improved from 5.84698 to 4.06351, saving model to ./model/best007-4.0635.hdf5\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.06351 to 2.57097, saving model to ./model/best008-2.5710.hdf5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.57097\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.57097 to 2.02539, saving model to ./model/best010-2.0254.hdf5\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.02539 to 1.80863, saving model to ./model/best011-1.8086.hdf5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.80863\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.80863\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.80863 to 0.74445, saving model to ./model/best014-0.7444.hdf5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.74445\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.74445\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.74445 to 0.58752, saving model to ./model/best017-0.5875.hdf5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.58752\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.58752\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.58752 to 0.41917, saving model to ./model/best020-0.4192.hdf5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.41917\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.41917\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.41917 to 0.34002, saving model to ./model/best023-0.3400.hdf5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.34002\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.34002\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.34002 to 0.27188, saving model to ./model/best026-0.2719.hdf5\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.27188\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.27188\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.27188 to 0.24995, saving model to ./model/best029-0.2500.hdf5\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.24995\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.24995\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.24995 to 0.24184, saving model to ./model/best032-0.2418.hdf5\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.24184\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.24184\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.24184\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.24184\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.24184\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.24184\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.24184\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.24184\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.24184\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.24184\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.24184\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.24184\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.24184\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.24184\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.24184\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.24184\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.24184\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.24184\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.24184\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.24184\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.24184 to 0.23445, saving model to ./model/best053-0.2345.hdf5\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.23445 to 0.23214, saving model to ./model/best054-0.2321.hdf5\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.23214 to 0.22934, saving model to ./model/best055-0.2293.hdf5\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.22934\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.22934 to 0.22684, saving model to ./model/best057-0.2268.hdf5\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.22684 to 0.22435, saving model to ./model/best058-0.2243.hdf5\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.22435 to 0.22371, saving model to ./model/best059-0.2237.hdf5\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.22371 to 0.22235, saving model to ./model/best060-0.2223.hdf5\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.22235 to 0.22129, saving model to ./model/best061-0.2213.hdf5\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.22129 to 0.22077, saving model to ./model/best062-0.2208.hdf5\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.22077 to 0.22068, saving model to ./model/best063-0.2207.hdf5\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.22068 to 0.22068, saving model to ./model/best064-0.2207.hdf5\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.22068\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.22068\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.22068\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.22068 to 0.22049, saving model to ./model/best068-0.2205.hdf5\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.22049\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.22049\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.22049\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.22049 to 0.22040, saving model to ./model/best072-0.2204.hdf5\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.22040 to 0.21976, saving model to ./model/best073-0.2198.hdf5\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.21976 to 0.21951, saving model to ./model/best074-0.2195.hdf5\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.21951\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.21951\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.21951 to 0.21898, saving model to ./model/best077-0.2190.hdf5\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.21898 to 0.21846, saving model to ./model/best078-0.2185.hdf5\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.21846 to 0.21807, saving model to ./model/best079-0.2181.hdf5\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.21807 to 0.21774, saving model to ./model/best080-0.2177.hdf5\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.21774 to 0.21723, saving model to ./model/best081-0.2172.hdf5\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.21723\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.21723\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.21723 to 0.21593, saving model to ./model/best084-0.2159.hdf5\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.21593 to 0.21504, saving model to ./model/best085-0.2150.hdf5\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.21504 to 0.21445, saving model to ./model/best086-0.2145.hdf5\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.21445 to 0.21418, saving model to ./model/best087-0.2142.hdf5\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.21418\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.21418\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.21418 to 0.21320, saving model to ./model/best090-0.2132.hdf5\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.21320 to 0.21298, saving model to ./model/best091-0.2130.hdf5\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.21298 to 0.21280, saving model to ./model/best092-0.2128.hdf5\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.21280\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.21280 to 0.21278, saving model to ./model/best094-0.2128.hdf5\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.21278 to 0.21199, saving model to ./model/best095-0.2120.hdf5\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.21199 to 0.21165, saving model to ./model/best096-0.2117.hdf5\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.21165\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.21165\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.21165\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.21165 to 0.21077, saving model to ./model/best100-0.2108.hdf5\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.21077 to 0.21044, saving model to ./model/best101-0.2104.hdf5\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.21044 to 0.21018, saving model to ./model/best102-0.2102.hdf5\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.21018 to 0.20994, saving model to ./model/best103-0.2099.hdf5\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.20994\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.20994\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.20994 to 0.20936, saving model to ./model/best106-0.2094.hdf5\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.20936\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.20936 to 0.20889, saving model to ./model/best108-0.2089.hdf5\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.20889\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.20889\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.20889 to 0.20835, saving model to ./model/best111-0.2084.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00112: val_loss did not improve from 0.20835\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.20835 to 0.20702, saving model to ./model/best113-0.2070.hdf5\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.20702\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.20702 to 0.20622, saving model to ./model/best116-0.2062.hdf5\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.20622 to 0.20588, saving model to ./model/best117-0.2059.hdf5\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.20588 to 0.20540, saving model to ./model/best118-0.2054.hdf5\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.20540\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.20540 to 0.20499, saving model to ./model/best120-0.2050.hdf5\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.20499 to 0.20453, saving model to ./model/best121-0.2045.hdf5\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.20453 to 0.20445, saving model to ./model/best122-0.2044.hdf5\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.20445\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.20445 to 0.20360, saving model to ./model/best124-0.2036.hdf5\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.20360 to 0.20344, saving model to ./model/best125-0.2034.hdf5\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.20344 to 0.20299, saving model to ./model/best126-0.2030.hdf5\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.20299 to 0.20279, saving model to ./model/best127-0.2028.hdf5\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.20279 to 0.20269, saving model to ./model/best128-0.2027.hdf5\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.20269\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.20269 to 0.20183, saving model to ./model/best130-0.2018.hdf5\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.20183\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.20183 to 0.20108, saving model to ./model/best132-0.2011.hdf5\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.20108 to 0.20066, saving model to ./model/best133-0.2007.hdf5\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.20066\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.20066 to 0.20022, saving model to ./model/best135-0.2002.hdf5\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.20022\n",
      "\n",
      "Epoch 00137: val_loss improved from 0.20022 to 0.19930, saving model to ./model/best137-0.1993.hdf5\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.19930\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.19930\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.19930 to 0.19863, saving model to ./model/best140-0.1986.hdf5\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.19863\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.19863 to 0.19781, saving model to ./model/best142-0.1978.hdf5\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.19781\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.19781\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.19781 to 0.19735, saving model to ./model/best145-0.1973.hdf5\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.19735\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.19735 to 0.19681, saving model to ./model/best147-0.1968.hdf5\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.19681\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.19681 to 0.19571, saving model to ./model/best149-0.1957.hdf5\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.19571\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.19571\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.19571\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.19571 to 0.19513, saving model to ./model/best153-0.1951.hdf5\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.19513\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.19513\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.19513\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.19513 to 0.19476, saving model to ./model/best157-0.1948.hdf5\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.19476\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.19476\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.19476\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.19476 to 0.19372, saving model to ./model/best161-0.1937.hdf5\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.19372\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.19372 to 0.19305, saving model to ./model/best163-0.1930.hdf5\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.19305\n",
      "\n",
      "Epoch 00165: val_loss improved from 0.19305 to 0.19264, saving model to ./model/best165-0.1926.hdf5\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.19264\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.19264\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.19264\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.19264 to 0.19112, saving model to ./model/best169-0.1911.hdf5\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.19112\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.19112 to 0.19073, saving model to ./model/best171-0.1907.hdf5\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.19073\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.19073\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.19073 to 0.19024, saving model to ./model/best174-0.1902.hdf5\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.19024 to 0.18920, saving model to ./model/best175-0.1892.hdf5\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.18920\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.18920 to 0.18847, saving model to ./model/best177-0.1885.hdf5\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.18847 to 0.18816, saving model to ./model/best178-0.1882.hdf5\n",
      "\n",
      "Epoch 00179: val_loss improved from 0.18816 to 0.18798, saving model to ./model/best179-0.1880.hdf5\n",
      "\n",
      "Epoch 00180: val_loss improved from 0.18798 to 0.18760, saving model to ./model/best180-0.1876.hdf5\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.18760 to 0.18749, saving model to ./model/best181-0.1875.hdf5\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.18749\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.18749\n",
      "\n",
      "Epoch 00184: val_loss improved from 0.18749 to 0.18701, saving model to ./model/best184-0.1870.hdf5\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.18701\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.18701 to 0.18689, saving model to ./model/best186-0.1869.hdf5\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.18689\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.18689\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.18689 to 0.18545, saving model to ./model/best189-0.1854.hdf5\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.18545\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.18545\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.18545\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.18545 to 0.18420, saving model to ./model/best193-0.1842.hdf5\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.18420\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.18420\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.18420\n",
      "\n",
      "Epoch 00197: val_loss improved from 0.18420 to 0.18376, saving model to ./model/best197-0.1838.hdf5\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.18376 to 0.18318, saving model to ./model/best198-0.1832.hdf5\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.18318\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.18318\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, validation_split=0.2, epochs=200, batch_size=200, \n",
    "          verbose=0, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('model/best198-0.1832.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 - 0s - loss: 0.1133 - accuracy: 0.9561\n",
      "\n",
      " Accuracy: 0.9561\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(x_test, y_test, verbose=2)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vloss=history.history['val_loss']\n",
    "y_acc=history.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsMAAAHWCAYAAACSf4T3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfzklEQVR4nO3de4x0510f8O+PXYJKgCZtDApJXAxySF1ULt6Gm0CpKMGOKlwqqGIQl7SS18qmAlWVkpQ/QFSoXEqFKFv8pmABEiTQAsVFbgJVW/JHG+p9aUjiBIMxIXES5dJUUJqq0b48/WPmxePNzOyZnevu8/lIqzlz5sw5zzx7Zua7v33OOdVaCwAA9OiTtt0AAADYFmEYAIBuCcMAAHRLGAYAoFvCMAAA3RKGAQDo1rlhuKoerKoPVdU7ZjxeVfVjVfV4Vb2tqr5k9c0EAIDVG1IZ/ukkd815/O4kt49/7kvyE8s3CwAA1u/cMNxae3OSj85Z5J4kP9tG3pLkWVX13FU1EAAA1mUVY4afl+S9E/efHM8DAICdtr+CddSUeVOv8VxV92U0lCLPfOYz73zRi160gs0DAMBs169f/0hr7ZZpj60iDD+Z5AUT95+f5P3TFmytvS7J65Lk4OCgnZycrGDzAAAwW1X90azHVjFM4qEk3zY+q8SXJfnj1toHVrBeAABYq3Mrw1X1+iQvSfKcqnoyyfck+eQkaa09kOThJC9L8niSjyV5xboaCwAAq3RuGG6t3XvO4y3J0cpaBAAAG+IKdAAAdEsYBgCgW8IwAADdEoYBAOiWMAwAQLeEYQAAuiUMAwDQLWEYAIBuCcMAAHRLGAYAoFvCMAAA3RKGAQDoljAMAEC3hGEAALolDAMA0C1hGACAbgnDAAB0SxgGAKBbwjAA0KWjo2R/f3Tbo3W//svSv8IwALCwyaCza6FnaHuuXUtu3BjdruM1DOmjbfbd5OsfYtG2Lrr+rWmtbeXnzjvvbADQi1e+srW9vdHtVdju3l5ryeh2cnpV7ZhcftHnDmnP2W0Mfc605w5px+T0rO0OWefZZdbdr4u2dZnf2zolOWkzMqkwDAAbsGjYWsa6Q97Z5RYNhssEyVW1Z97zh1gmGA4JxkO2O7Qd855/0dc5az2b3M8XIQwDwJZtskq2aECa9dxVWjT0LRO2N2Edf3AsWxled4V+Va9hG4RhAFiTi/zbetFlhgTDRZdZ9LmrtI5trHKduxYAL1Kt38T2Vr3ddRKGuVIuw5tul+iv2bb1xbHpdS4Tzjb95b+OULbMF/uQ9lzk39azKnizqnxDhgxc5F/sQ567zO/ksn7+LNPH63jNy1a91/3fgW31yyKEYa6UXR2PtKuW+Xdpa+uvGG0z/CzaN7PaOiTYLNq+Ve7ns17nkNcw67mr6rt5bV3mX+nLtHVICJ33ehbdN4a0ddHpZfprlb+TZX6HQ5Zfl2X6eB3fUct+dq/j97Bo+7b93S0Mc6Vs+6/LVVvm9Sz6xXGRD6NFw8Ci0+tY/9nXuUxgWrSts/puVYFn6OtZR38v83rmBap1vJ519P1F3kuLhoRFPw+WCRgXeQ3reD3z9o1VrH/e+2eIRft4Vd9Ry65nmd/DOmz7u1sY5tLb9pvorHntWeaDdpkv5CFfWEPnryqELBpUVrX+s69nyJftMoFx1noWDZKz2nZ2/qL9MaR987a3yP5zkb5bNAAt+p5ZtK1DrPIzaZl1beu5q1zvomF70T9K5r1/1vF6VmXZcLrM7+EqEoa59Nb9F+ui5rVnmQ/aZULfMl/sF/myWKaqtugX3rLrXNW2L9KX09a56LYu8sfKLgSsoetZ9+9kk+tksxbdd4Y+ZxdclnZeFsIwO+ey/MW6bPAast5Ft7dMqBq6vA/hi9N3ALtnXhiu0eObd3Bw0E5OTraybbZvf390ica9veT0dPXrPzoaXf7x8DA5Pp4+Pzl/mZuXktzbGz02q82Lbm9yvet4/UOs+3cAALuiqq631g6mPfZJm24MJKNAuLf3VEhchclrps+63vzk/FnXTJ+cP9nOeW0esq5Z692WXWgDAGybyjAbMatyusr1TlZbb96fNz85f3poW5epRAMA6zWvMiwMsxHr+pf85Hong+6sQDorhBoyAABXl2ESbN26/iU/ud7j41GQPRt4Z83fRPsAgN2mMszaLDs0Yl1DKwCAvqgMsxWzDiqbZ9bBbgAA6yAMcyGToXWWoUMPZgVgQxcAgHUzTIILmXXA2UWGNgw5CA4A4KIMk2DlZlVtLzK0YchBcAAA66AyzEo56A0A2DUqw6zV5JhflV0A4DIRhq+SIUe1rWH1zvoAAFxWwvBVsuZUOmv1zvoAAFxWwvBVsuZUOmv1hkYAAJeVA+gAALjSHEAHAABTCMMAAHRLGAYAoFvCMAAA3RKGAQDoljAMAEC3hGEAALolDAMA0C1hGACAbgnDAAB0SxgGAKBbwjBJkqOjZH9/dAsA0AthmCTJtWvJjRujWwCAXgjDJEkOD5O9vdEtAEAvhOGOTQ6NOD5OTk9HtwAAvRCGO2ZoBADQO2G4Y4ZGAAC9E4Y7Y2gEAMBThOHOGBoBAPAUYbgzhkYAADxFGO6AoREAANMJwx0wNAIAYDphuAOGRgAATFetta1s+ODgoJ2cnGxl2wAA9KOqrrfWDqY9pjIMAEC3hGEAALolDAMA0C1hGACAbgnDzDV5jmIAgKtGGGYu5ygGAK4yYZi5nKMYALjKnGcYAIArbenzDFfVXVX1WFU9XlWvmfL4X6yqf19Vv1NVj1bVK5ZtNAAArNu5Ybiq9pIcJ7k7yR1J7q2qO84sdpTkna21L0zykiQ/UlXPWHFb2RAHzQEAvRhSGX5xksdba0+01j6e5A1J7jmzTEvy6VVVST4tyUeTnK60pWyMg+YAgF4MCcPPS/LeiftPjudN+vEkfzXJ+5O8Pcl3ttb+bCUtZOMcNAcA9GJIGK4p884edfd1Sd6a5LOTfFGSH6+qz/iEFVXdV1UnVXXy4Q9/eOHGshnHx8np6egWAOAqGxKGn0zygon7z8+oAjzpFUl+uY08nuQPk7zo7Ipaa69rrR201g5uueWWi7YZAABWYkgYfiTJ7VV12/iguJcneejMMu9J8jVJUlWfleTzkzyxyoYCAMCq7Z+3QGvttKpeleRNSfaSPNhae7Sq7h8//kCSf5rkp6vq7RkNq3h1a+0ja2w3AAAs7dwwnCSttYeTPHxm3gMT0+9P8tLVNg0AANbL5ZgBAOiWMAwAQLeEYQAAuiUMAwDQLWEYAIBuCcMAAHRLGAYAoFvCMAAA3RKGAQDoljAMAEC3hGEAALolDAMA0C1hGACAbgnDAAB0SxgGAKBbwjAAAN0ShgEA6JYwDABAt4ThXXJ0lOzvj26XXwwAgHNUa20rGz44OGgnJydb2fbO2t9PbtxI9vaS09NlFwMAIElVXW+tHUx7TGV4lxwejhLu4eEqFgMA4BwqwwAAXGkqwwAAMIUwDABAt4RhAAC6JQwDANAtYRgAgG4JwwAAdEsYBgCgW8IwAADdEoYBAOiWMAwAQLeEYQAAuiUMAwDQLWEYAIBuCcMAAHRLGAYAoFvCMAAA3RKGL7mjo2R/f3QLAMBihOFL7tq15MaN0S0AAIsRhi+5w8Nkb290CwDAYqq1tpUNHxwctJOTk61sGwCAflTV9dbawbTHVIYvCWODAQBWTxi+JIwNBgBYPWH4kjA2GABg9YwZBgDgSjNm+BIyRhgAYP2E4R1ljDAAwPoJwztq2THCKssAAOczZviK2t8fVZb39pLT0223BgBge4wZ7pCzTwAAnE9lGACAK01lGAAAphCGAQDoljAMAEC3hGEAALolDAMA0C1hGACAbgnDAAB0SxgGAKBbwjAAAN0ShgEA6JYwDABAt4RhAAC6JQwDANAtYRgAgG4JwwAAdEsYBgCgW8IwAADdEoYBAOiWMAwAQLeEYQAAutVdGD46Svb3R7eT0wAA9Kdaa1vZ8MHBQTs5Odn4dvf3kxs3kr290f2b06enG28KAAAbUFXXW2sH0x7rrjJ8eDgKv4eHT58GAKA/3VWGAQDoi8owAABMIQwDANCtQWG4qu6qqseq6vGqes2MZV5SVW+tqker6jdX20wAAFi9/fMWqKq9JMdJvjbJk0keqaqHWmvvnFjmWUn+VZK7WmvvqarPXFeDAQBgVYZUhl+c5PHW2hOttY8neUOSe84s881Jfrm19p4kaa19aLXNBACA1RsShp+X5L0T958cz5v0wiTPrqr/UlXXq+rbVtVAAABYl3OHSSSpKfPOno9tP8mdSb4myV9I8t+q6i2ttd972oqq7ktyX5Lceuuti7cWAABWaEhl+MkkL5i4//wk75+yzBtba/+ntfaRJG9O8oVnV9Rae11r7aC1dnDLLbdctM0AALASQ8LwI0lur6rbquoZSV6e5KEzy/xqkq+qqv2q+tQkX5rkXattKgAArNa5wyRaa6dV9aokb0qyl+TB1tqjVXX/+PEHWmvvqqo3Jnlbkj9L8pOttXess+EAALCsQecZbq093Fp7YWvt81pr3z+e90Br7YGJZX64tXZHa+0LWms/uq4Gk+ToKNnfH90CAHBhrkB3GV27lty4MboFAODChOHL6PAw2dsb3QIAcGHV2tmzpG3GwcFBOzk52cq2AQDoR1Vdb60dTHtMZRgAgG4Jw2vkODcAgN0mDK+R49wAAHabMLxGO3mcm3I1AMCfcwBdD46ORuXpw8OnytV7e8np6bZbBgCwdg6g693keI2dLFcDAGyHMLwhWx2dMBmAj49HFeHj4y00BABgtxgmsSH7+0YnAABsg2ESO8DoBACA3aMyDADAlaYyDAAAUwjDAAB0SxgGAKBbwjAAAN0ShgEA6JYwDABAt4ThK2SrV7kDALiEhOEr5Nq10VXurl3bdksAAC4HYfgKcZU7AIDFuAIdAABXmivQbZBxuwAAl4cwvGLG7QIAXB7C8IoZtwsAcHkYMwwAwJVmzDAAAEwhDAMA0C1hGACAbgnDAAB0SxgGAKBbwjAAAN0ShgEA6JYwDABAt4RhAAC6JQwDANAtYRgAgG4JwwAAdEsYBgCgW8IwAADdEoYBAOiWMAwAQLeEYQAAuiUMAwDQLWEYAIBuCcMAAHRLGAYAoFvCMAAA3RKGAQDoljAMAEC3hGEAALolDAMA0C1hGACAbgnDAAB0SxgGAKBbwjAAAN0ShgEA6JYwDABAt4RhAAC6JQwDANAtYZhPdHSU7O+PbgEArjBhmE907Vpy48boFgDgChOG+USHh8ne3ugWAOAKq9baVjZ8cHDQTk5OtrJtAAD6UVXXW2sH0x5TGQYAoFvCMAAA3RKGAQDoljAMAEC3hGEAALolDAMA0C1hGACAbgnDAAB0SxjeIUdHyf7+6Ha3VgYAcDW5At0O2d9PbtwYXQn59HSXVgYAcHm5At0lcXg4yq6Hh7u2MgCAq0llGACAK01lGAAAphgUhqvqrqp6rKoer6rXzFnub1TVjar6xtU1EQAA1uPcMFxVe0mOk9yd5I4k91bVHTOW+8Ekb1p1IwEAYB2GVIZfnOTx1toTrbWPJ3lDknumLPcPk/xSkg+tsH1silOxAQAdGhKGn5fkvRP3nxzP+3NV9bwk35DkgdU1jY26dm10KrZr17bdEgCAjRkShmvKvLOnoPjRJK9urd2Yu6Kq+6rqpKpOPvzhDw9tI5vgVGwAQIfOPbVaVX15ku9trX3d+P5rk6S19s8mlvnDPBWan5PkY0nua639u1nrdWo1AAA2Yd6p1fYHPP+RJLdX1W1J3pfk5Um+eXKB1tptExv76SS/Ni8IAwDALjg3DLfWTqvqVRmdJWIvyYOttUer6v7x48YJAwBwKQ06z3Br7eHW2gtba5/XWvv+8bwHpgXh1tp3tNb+7aobum5OpgAA0B9XoBtzMgUAgP4Iw2NOpgAA0J9zzyaxLs4mAQDAJsw7m4TK8DkmxxKvY1yxscoAANujMnyO/f3RWOK9vdH9m9OHh6PxxYeHyfHxatZ/erqaNgMA8BSV4QVNVmsnxxJPTi9zwN2s9QMAsFkqw1MMqdYeHV28MqwaDACwOSrDCxpSrT0+HgXZiwyRUA0GANgNKsMAAFxpKsMAADCFMAwAQLeEYQAAuiUMAwDQLWEYAIBuCcMAAHRLGAYAoFvCMAAA3RKGAQDoljAMAEC3hGEAALolDK/A0VGyvz+6vVQubcMBAFajWmtb2fDBwUE7OTnZyrZXbX8/uXEj2dtLTk+33ZoFXNqGAwAMV1XXW2sH0x5TGV6Bw8NRnjw83HZLFnRpGw4AsBoqwwAAXGkqwwAAMIUwzMU4+A4AuAKEYS7m2rXRwXfXrm27JQAAFyYMczEOvgMArgAH0AEAcKU5gA4AAKYQhgEA6JYwDABAt4RhAAC6JQwDANAtYRgAgG4JwwznqnMAwBUjDDOcq84BAFeMMMxwrjoHAFwxrkAHAMCV5gp0AAAwhTAMAEC3hGEAALolDLN9TtkGAGyJMMz2OWUbALAlwjDb55RtAMCWOLUaAABXmlOrAQDAFMIwAADdEoYBAOiWMMz6bPqUaU7RBgAsSBhmvmUC5qZPmeYUbQDAgoRh5lsmYG7ilGmTYd0p2gCABTm1GvMdHY2C8OFhcny8e+vd3x+F9b295PR0de0DAK4Mp1bj4o6PRyFzlUE4Wd2QBtVgAGAJwjDbsWiInTV2eVZYX3Ss8+TyDsQDgG4YJsFumTV8YtHhEJPLHx6ePyRjcvnE0AsAuEIMk+DymDV8YrKSPKRyO7n85DpnVYAnlzf0AgC6oTLMbpmsDCerqRJPrvNmMFYBBoBuqAxzeUyOAR5SJV50nSrAAMAEYZjlrevgs1lhdZkzXEw+d8h6HEwHAFeaYRIs7yoffLbogXgAwM4xTIL1uspDD4YciJeoIAPAJaUyDEPNOhDv9FQFGQB2mMow23cVKqezDsRLhleQAYCdojLMZix6OrTLbF4FGQDYOJVhtu+qjSWeZ14FGQDYKSrDsCmzLjUNAKyVyjDsAmOJAWDnCMOwKUMOshOSAWCj9rfdAOjGzave3XT2ILubl52enDasAgDWSmUYtmHWQXZO0QYAGyUMw7ZNBuNZIXkdwyoMzwAAZ5OAS2HWuYuT6Ve+S86fnrWe09Onb2/IulY1varhIM7cAcCEeWeTEIbhspkVVGeF21nTs8Lz8fHTL5IyZF2rml400M+adrETACYIw9CDRau58yqm26oMLxroZ01Phurj49mvZ1YfLLr8rOeqSgPsBGEYuBxWFcLPhtBZle5ZlehlhqJMPvdsKF/k9a8ySAvoQOfmheG01rbyc+eddzaAjXjlK1vb2xvdTk7v7bWWjG4npxddftZzh6xz0eXnvZ4h653VL0PmA1xSSU7ajEwqDAP9mhc4F1l+SKhcJkifDbND1rVouF8mxAPsuKXDcJK7kjyW5PEkr5ny+Lckedv4578m+cLz1ikMA11ZJkgvWxmetGgAXiZsD22bkA2s2VJhOMlekj9I8rlJnpHkd5LccWaZr0jy7PH03Ul+67z1CsMAW7Do0IhFh2EsOt3aekL2oqF60yFc6N8cfU1bPgx/eZI3Tdx/bZLXzln+2Uned956hWGAK2JdleFlQvY6xmtfZIz1ohX6desxGM4aM09Xlg3D35jkJyfuf2uSH5+z/D+eXH7WjzAMwFyrqgyvarz2kOXPtnuZsdsXGUIzbZlJs4LhRQL9osus6rm7XvVnJy0bhr9pShj+lzOW/ZtJ3pXkL894/L4kJ0lObr311k29fgB6tkyld9EA29rw0Dxte8uE9WUC9tDXM8uile4h/TJkW7PWCWdsZJhEkr8+Hlv8wvPW2VSGAbjMLlJJXTQkrqrCPCQ8Dq10r2rIyZDwvKqqN7Tlw/B+kieS3DZxAN1fO7PMreMzTXzFeeu7+SMMA8BAywxVWOWQiVlV6VnPH1LFHvLcZar48ywzFGVon7ETVnFqtZcl+b1x5fe7x/PuT3L/ePonk/yvJG8d/8zcYBOGAWD7LhLglgmJQ5ZftLq96PyzbVpmKMrQ7V3Usr8fnsZFNwCA3beqIRNDq9urGt6xjulFx33Pe23L9P0VIQwDALS2ujHdQ6rSy0wPqVSfbdMywX1IdfsiQ1F2xLwwXKPHN+/g4KCdnJxsZdsAAEs5OkquXUsOD5Pj4+nzk4tPD1nn8XGyv5/cuJHs7SWnp9Ofc+3aU8sk06dvLjevTbPWM+S5s17bhlTV9dbawdTHhGEAgEtqViiftUxyfkCdDNjJ+aF3SNienJ4M7RsiDAMAMMy8SvQiy8+aVhkeEYYBANiEeWH4kzbdGAAA2BXCMAAA3RKGAQDoljAMAEC3hGEAALolDAMA0C1hGACAbgnDAAB0SxgGAKBbwjAAAN0ShgEA6JYwDABAt4RhAAC6JQwDANAtYRgAgG4JwwAAdEsYBgCgW8IwAADdEoYBAOiWMAwAQLeEYQAAuiUMAwDQLWEYAIBuCcMAAHRLGAYAoFvCMAAA3RKGAQDoljAMAEC3hGEAALolDAMA0C1hGACAbgnDAAB0SxgGAKBbwjAAAN0ShgEA6JYwDABAt4RhAAC6JQwDANAtYRgAgG4JwwAAdEsYBgCgW8IwAADdEoYBAOiWMAwAQLeEYQAAuiUMAwDQLWEYAIBuCcMAAHRLGAYAoFvCMAAA3RKGAQDoljAMAEC3hGEAALolDAMA0C1hGACAbgnDAAB0SxgGAKBbwjAAAN0ShgEA6JYwDABAt4RhAAC6JQwDANAtYRgAgG4JwwAAdEsYBgCgW8IwAADdEoYBAOiWMAwAQLeEYQAAuiUMAwDQLWEYAIBuCcMAAHRLGAYAoFuDwnBV3VVVj1XV41X1mimPV1X92Pjxt1XVl6y+qQAAsFrnhuGq2ktynOTuJHckubeq7jiz2N1Jbh//3JfkJ1bcTgAAWLkhleEXJ3m8tfZEa+3jSd6Q5J4zy9yT5GfbyFuSPKuqnrvitgIAwEoNCcPPS/LeiftPjuctugwAAOyU/QHL1JR57QLLpKruy2gYRZL8aVU9NmD76/CcJB/Z0rYvI/21OH22GP21OH22GP21GP21OH22mE3311+Z9cCQMPxkkhdM3H9+kvdfYJm01l6X5HUDtrlWVXXSWjvYdjsuC/21OH22GP21OH22GP21GP21OH22mF3qryHDJB5JcntV3VZVz0jy8iQPnVnmoSTfNj6rxJcl+ePW2gdW3FYAAFipcyvDrbXTqnpVkjcl2UvyYGvt0aq6f/z4A0keTvKyJI8n+ViSV6yvyQAAsBpDhkmktfZwRoF3ct4DE9MtydFqm7ZWWx+qccnor8Xps8Xor8Xps8Xor8Xor8Xps8XsTH/VKMcCAEB/XI4ZAIBudRWGz7usdO+q6gVV9Z+r6l1V9WhVfed4/vdW1fuq6q3jn5dtu627pKreXVVvH/fNyXjeX6qq36iq3x/fPnvb7dwFVfX5E/vRW6vqT6rqu+xjT1dVD1bVh6rqHRPzZu5TVfXa8efaY1X1ddtp9XbN6LMfrqrfraq3VdWvVNWzxvM/p6r+78T+9sDsNV9NM/pr5vvQPjazz35hor/eXVVvHc+3j83OFDv3WdbNMInxZaV/L8nXZnQquEeS3Ntae+dWG7ZDxlcNfG5r7ber6tOTXE/yd5L8vSR/2lr751tt4I6qqncnOWitfWRi3g8l+Whr7QfGf3g9u7X26m21cReN35PvS/KlGR10ax8bq6qvTvKnGV3Z8wvG86buU1V1R5LXZ3S10M9O8h+TvLC1dmNLzd+KGX320iT/aXwg+A8mybjPPifJr91crkcz+ut7M+V9aB8bmdZnZx7/kYzOpvV99rG5meI7smOfZT1VhodcVrprrbUPtNZ+ezz9v5O8K64keFH3JPmZ8fTPZPQBwNN9TZI/aK390bYbsmtaa29O8tEzs2ftU/ckeUNr7f+11v4wo7P6vHgjDd0h0/qstfbrrbXT8d23ZHQOfDJzH5vFPpb5fVZVlVHh6PUbbdQOm5Mpdu6zrKcw7JLRCxj/VfvFSX5rPOtV4381Puhf/p+gJfn1qrpeo6ssJsln3TzX9vj2M7fWut318jz9i8M+Nt+sfcpn2zB/P8l/mLh/W1X9j6r6zar6qm01agdNex/ax873VUk+2Fr7/Yl59rGxM5li5z7LegrDgy4ZTVJVn5bkl5J8V2vtT5L8RJLPS/JFST6Q5Ee22Lxd9JWttS9JcneSo/G/0pijRhfw+fok/2Y8yz52cT7bzlFV353kNMnPjWd9IMmtrbUvTvKPkvx8VX3Gttq3Q2a9D+1j57s3T//j3j42NiVTzFx0yryN7Gc9heFBl4zuXVV9ckY77c+11n45SVprH2yt3Wit/VmSf50O/z02T2vt/ePbDyX5lYz654Pj8VI3x019aHst3El3J/nt1toHE/vYQLP2KZ9tc1TVtyf520m+ZXxO/Iz/Dfs/x9PXk/xBkhdur5W7Yc770D42R1XtJ/m7SX7h5jz72Mi0TJEd/CzrKQwPuax018Zjnn4qybtaa/9iYv5zJxb7hiTvOPvcXlXVM8cHBqSqnpnkpRn1z0NJvn282Lcn+dXttHBnPa2KYh8bZNY+9VCSl1fVp1TVbUluT/Lft9C+nVNVdyV5dZKvb619bGL+LeMDOFNVn5tRnz2xnVbujjnvQ/vYfH8rye+21p68OcM+NjtTZAc/ywZdge4qmHVZ6S03a9d8ZZJvTfL2m6eHSfJPktxbVV+U0b8r3p3kcDvN20mfleRXRu/57Cf5+dbaG6vqkSS/WFX/IMl7knzTFtu4U6rqUzM6q8vkfvRD9rGnVNXrk7wkyXOq6skk35PkBzJln2qtPVpVv5jknRkNBTjq7Sj/ZGafvTbJpyT5jfF79C2ttfuTfHWS76uq0yQ3ktzfWht6MNmVMKO/XjLtfWgfG5nWZ621n8onHv+Q2MeS2Zli5z7Lujm1GgAAnNXTMAkAAHgaYRgAgG4JwwAAdEsYBgCgW8IwAADdEoYBAOiWMAwAQLeEYQAAuvX/AXq6pDjMMeRtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_len = np.arange(len(y_acc))\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=2)\n",
    "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=2)\n",
    "plt.ylim((0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vacc=history.history['val_accuracy']\n",
    "y_loss=history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_len = np.arange(len(y_acc))\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(x_len, y_loss, \"o\", c=\"red\", markersize=1)\n",
    "plt.plot(x_len, y_vacc, \"o\", c=\"blue\", markersize=1)\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"orange\", markersize=1)\n",
    "plt.plot(x_len, y_acc, \"o\", c=\"green\", markersize=1)\n",
    "plt.ylim((0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
