{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dogs vs Cats\n",
    "## Kaggle Dataset의 전부를 이용한 개, 고양이 구분\n",
    "### Dog Image: 12,500개, Cat Image: 12,500개, 총 25,000개\n",
    "### 출처: [pontoregende GitHub](https://github.com/pontorezende/Dogs-vs-Cats-Redux-with-CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import cv2, os, random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='dogs-vs-cats/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## used for resize and in our model\n",
    "ROW, COL = 96, 96\n",
    "\n",
    "dogs, cats = [], []\n",
    "y_dogs, y_cats = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definition to load all our dog images\n",
    "def load_dogs():\n",
    "    print('Loading all dog images\\n')\n",
    "    dog_path = os.path.join(path, 'dog*')\n",
    "    for dog_img in glob(dog_path):\n",
    "        dog = cv2.imread(dog_img)\n",
    "        dog = cv2.cvtColor(dog, cv2.COLOR_BGR2GRAY)\n",
    "        dog = cv2.resize(dog, (ROW, COL))\n",
    "        dog = image.img_to_array(dog)\n",
    "        dogs.append(dog)\n",
    "    print('All dog images loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Definition to load all our cat images\n",
    "def load_cats():\n",
    "    print('Loading all cat images\\n')\n",
    "    cat_path = os.path.join(path, 'cat*')\n",
    "    for cat_img in glob(cat_path):\n",
    "        cat = cv2.imread(cat_img)\n",
    "        cat = cv2.cvtColor(cat, cv2.COLOR_BGR2GRAY)\n",
    "        cat = cv2.resize(cat, (ROW, COL))\n",
    "        cat = image.img_to_array(cat)\n",
    "        cats.append(cat)\n",
    "    print('All cat images loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all dog images\n",
      "\n",
      "All dog images loaded\n"
     ]
    }
   ],
   "source": [
    "load_dogs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all cat images\n",
      "\n",
      "All cat images loaded\n"
     ]
    }
   ],
   "source": [
    "load_cats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['dog', 'cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in case we want to see if our images was saved correctly in arrays we can use those codes\n",
    "def show_dogs():\n",
    "    plt.figure(figsize=(12,8))    \n",
    "    for i in range(5):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        img = image.array_to_img(random.choice(dogs))\n",
    "        plt.imshow(img)\n",
    "        \n",
    "        plt.axis('off')\n",
    "        plt.title('Supposed to be a {}'.format(classes[0]))        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cats():\n",
    "    plt.figure(figsize=(12,8))\n",
    "    for i in range(5):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        img = image.array_to_img(random.choice(cats))\n",
    "        plt.imshow(img)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.title('Supposed to be a {}'.format(classes[1]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## just change the labels for 0 and  1\n",
    "y_dogs = [1 for item in enumerate(dogs)]\n",
    "y_cats = [0 for item in enumerate(cats)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting everything to Numpy array to fit in our model\n",
    "## them creating a X and target file like we used to see\n",
    "## in Machine and Deep Learning models\n",
    "dogs = np.asarray(dogs).astype('float32')\n",
    "cats = np.asarray(cats).astype('float32')\n",
    "y_dogs = np.asarray(y_dogs).astype('int32')\n",
    "y_cats = np.asarray(y_cats).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit values between 0 and 1\n",
    "dogs /= 255\n",
    "cats /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((dogs,cats), axis=0)\n",
    "y = np.concatenate((y_dogs, y_cats), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_CHANNEL = 1\n",
    "BATCH_SIZE = 128\n",
    "N_EPOCH = 10\n",
    "VERBOSE = 1\n",
    "VALIDAION_SPLIT = .2\n",
    "OPTIM = Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 96, 96, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 96, 96, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 48, 48, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 36864)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               18874880  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 18,940,385\n",
      "Trainable params: 18,940,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Here is our model as a CNN\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), padding='same', input_shape=(ROW, COL, IMG_CHANNEL), activation='relu'),\n",
    "    Conv2D(32, (3,3), padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(.25),\n",
    "    Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "    Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(.25),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=OPTIM, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = \"model/dogs_vs_cats-cnn-{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=modelpath, monitor='val_loss', \n",
    "                               verbose=1, shuffle=True, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "19968/20000 [============================>.] - ETA: 0s - loss: 0.6589 - accuracy: 0.6211\n",
      "Epoch 00001: val_loss improved from inf to 0.90882, saving model to model/dogs_vs_cats-cnn-01-0.9088.hdf5\n",
      "20000/20000 [==============================] - 535s 27ms/sample - loss: 0.6589 - accuracy: 0.6213 - val_loss: 0.9088 - val_accuracy: 0.1744\n",
      "Epoch 2/10\n",
      "19968/20000 [============================>.] - ETA: 1s - loss: 0.5743 - accuracy: 0.6908\n",
      "Epoch 00002: val_loss improved from 0.90882 to 0.77116, saving model to model/dogs_vs_cats-cnn-02-0.7712.hdf5\n",
      "20000/20000 [==============================] - 737s 37ms/sample - loss: 0.5743 - accuracy: 0.6908 - val_loss: 0.7712 - val_accuracy: 0.5184\n",
      "Epoch 3/10\n",
      "19968/20000 [============================>.] - ETA: 1s - loss: 0.5087 - accuracy: 0.7498\n",
      "Epoch 00003: val_loss improved from 0.77116 to 0.74975, saving model to model/dogs_vs_cats-cnn-03-0.7497.hdf5\n",
      "20000/20000 [==============================] - 737s 37ms/sample - loss: 0.5088 - accuracy: 0.7499 - val_loss: 0.7497 - val_accuracy: 0.6042\n",
      "Epoch 4/10\n",
      "19968/20000 [============================>.] - ETA: 1s - loss: 0.4587 - accuracy: 0.7833\n",
      "Epoch 00004: val_loss improved from 0.74975 to 0.51372, saving model to model/dogs_vs_cats-cnn-04-0.5137.hdf5\n",
      "20000/20000 [==============================] - 738s 37ms/sample - loss: 0.4587 - accuracy: 0.7832 - val_loss: 0.5137 - val_accuracy: 0.7538\n",
      "Epoch 5/10\n",
      "19968/20000 [============================>.] - ETA: 2s - loss: 0.4132 - accuracy: 0.8111 \n",
      "Epoch 00005: val_loss did not improve from 0.51372\n",
      "20000/20000 [==============================] - 1892s 95ms/sample - loss: 0.4133 - accuracy: 0.8110 - val_loss: 0.7025 - val_accuracy: 0.5914\n",
      "Epoch 6/10\n",
      "19968/20000 [============================>.] - ETA: 1s - loss: 0.3768 - accuracy: 0.8298\n",
      "Epoch 00006: val_loss did not improve from 0.51372\n",
      "20000/20000 [==============================] - 740s 37ms/sample - loss: 0.3768 - accuracy: 0.8298 - val_loss: 0.6453 - val_accuracy: 0.6690\n",
      "Epoch 7/10\n",
      "19968/20000 [============================>.] - ETA: 0s - loss: 0.3317 - accuracy: 0.8544\n",
      "Epoch 00007: val_loss improved from 0.51372 to 0.43639, saving model to model/dogs_vs_cats-cnn-07-0.4364.hdf5\n",
      "20000/20000 [==============================] - 596s 30ms/sample - loss: 0.3317 - accuracy: 0.8543 - val_loss: 0.4364 - val_accuracy: 0.7972\n",
      "Epoch 8/10\n",
      "19968/20000 [============================>.] - ETA: 0s - loss: 0.2924 - accuracy: 0.8726\n",
      "Epoch 00008: val_loss did not improve from 0.43639\n",
      "20000/20000 [==============================] - 532s 27ms/sample - loss: 0.2925 - accuracy: 0.8727 - val_loss: 0.6134 - val_accuracy: 0.7260\n",
      "Epoch 9/10\n",
      "19968/20000 [============================>.] - ETA: 1s - loss: 0.2536 - accuracy: 0.8941\n",
      "Epoch 00009: val_loss did not improve from 0.43639\n",
      "20000/20000 [==============================] - 716s 36ms/sample - loss: 0.2535 - accuracy: 0.8942 - val_loss: 0.5787 - val_accuracy: 0.7438\n",
      "Epoch 10/10\n",
      "19968/20000 [============================>.] - ETA: 1s - loss: 0.2076 - accuracy: 0.9153\n",
      "Epoch 00010: val_loss did not improve from 0.43639\n",
      "20000/20000 [==============================] - 732s 37ms/sample - loss: 0.2075 - accuracy: 0.9153 - val_loss: 0.5922 - val_accuracy: 0.7696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26460038c88>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## to save checkpoint to use latter\n",
    "model.fit(X, y, batch_size=BATCH_SIZE, epochs=N_EPOCH, validation_split=VALIDAION_SPLIT,\n",
    "          verbose=VERBOSE, shuffle=True, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "del model\n",
    "model = load_model('model/dogs_vs_cats-cnn-07-0.4364.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 - 170s - loss: 0.2935 - accuracy: 0.8798\n",
      "MODEL ACCURACY: 0.87980\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X, y, verbose=2)\n",
    "print('MODEL ACCURACY: %.5f' % scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
